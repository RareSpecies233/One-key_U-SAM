import torch

def check_torch_device():
    """
    æ£€æŸ¥PyTorchä½¿ç”¨çš„è®¡ç®—è®¾å¤‡ï¼ˆCPUæˆ–å…·ä½“åŠ é€Ÿå¹³å°ï¼‰
    """
    print("=" * 50)
    print("PyTorch è®¾å¤‡æ£€æµ‹ç»“æœ")
    print("=" * 50)
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print("-" * 50)
    
    # 1. æ£€æµ‹ CPUï¼ˆå§‹ç»ˆå¯ç”¨ï¼‰
    cpu_available = True
    print(f"âœ… CPU: å¯ç”¨ï¼ˆæ‰€æœ‰ç¯å¢ƒé»˜è®¤æ”¯æŒï¼‰")
    
    # 2. æ£€æµ‹ NVIDIA CUDAï¼ˆæœ€ä¸»æµçš„GPUåŠ é€Ÿå¹³å°ï¼Œæ”¯æŒWindows/Linuxï¼‰
    cuda_available = torch.cuda.is_available()
    if cuda_available:
        cuda_device_count = torch.cuda.device_count()
        current_cuda_device = torch.cuda.current_device()
        cuda_device_name = torch.cuda.get_device_name(current_cuda_device)
        cuda_version = torch.version.cuda
        print(f"âœ… CUDA: å¯ç”¨")
        print(f"   - CUDA ç‰ˆæœ¬: {cuda_version}")
        print(f"   - å¯ç”¨ GPU æ•°é‡: {cuda_device_count}")
        print(f"   - å½“å‰é»˜è®¤ GPU: {current_cuda_device}ï¼ˆåç§°ï¼š{cuda_device_name}ï¼‰")
    else:
        print(f"âŒ CUDA: ä¸å¯ç”¨ï¼ˆæœªå®‰è£…å¯¹åº”CUDAç‰ˆæœ¬ã€æ— NVIDIAæ˜¾å¡æˆ–é©±åŠ¨ä¸å…¼å®¹ï¼‰")
    
    # 3. æ£€æµ‹ Apple MPSï¼ˆè‹¹æœSiliconèŠ¯ç‰‡ï¼ˆM1/M2/M3ï¼‰çš„åŠ é€Ÿå¹³å°ï¼‰
    try:
        mps_available = torch.backends.mps.is_available()
        mps_built = torch.backends.mps.is_built()
    except AttributeError:
        # ä½ç‰ˆæœ¬PyTorchä¸æ”¯æŒMPSï¼Œç›´æ¥æ ‡è®°ä¸ºä¸å¯ç”¨
        mps_available = False
        mps_built = False
    
    if mps_available and mps_built:
        print(f"âœ… MPS: å¯ç”¨ï¼ˆApple Silicon èŠ¯ç‰‡ç¡¬ä»¶åŠ é€Ÿï¼‰")
    else:
        reason = []
        if not mps_built:
            reason.append("PyTorch ç¼–è¯‘æ—¶æœªå¯ç”¨ MPS æ”¯æŒ")
        if not mps_available:
            reason.append("é Apple Silicon èŠ¯ç‰‡æˆ–ç³»ç»Ÿç‰ˆæœ¬è¿‡ä½ï¼ˆéœ€macOS 12.3+ï¼‰")
        print(f"âŒ MPS: ä¸å¯ç”¨ï¼ˆ{'; '.join(reason)}ï¼‰")
    
    # 4. æ£€æµ‹å…¶ä»–å°ä¼—åŠ é€Ÿå¹³å°ï¼ˆå¯é€‰ï¼ŒæŒ‰éœ€å‚è€ƒï¼‰
    # æ£€æµ‹ Google TPU
    try:
        tpu_available = torch.backends.xla.is_available()
    except AttributeError:
        tpu_available = False
    print(f"{'âœ…' if tpu_available else 'âŒ'} TPU (XLA): {'å¯ç”¨ï¼ˆéœ€åœ¨Google Colab/TPUç¯å¢ƒä¸­ï¼‰' if tpu_available else 'ä¸å¯ç”¨'}")
    
    # æ£€æµ‹ Graphcore IPU
    try:
        ipu_available = torch.backends.ipu.is_available()
    except AttributeError:
        ipu_available = False
    print(f"{'âœ…' if ipu_available else 'âŒ'} IPU: {'å¯ç”¨ï¼ˆGraphcore ç¡¬ä»¶ç¯å¢ƒï¼‰' if ipu_available else 'ä¸å¯ç”¨'}")
    
    print("-" * 50)
    # 5. è¾“å‡º PyTorch é»˜è®¤ä½¿ç”¨çš„è®¾å¤‡
    if cuda_available:
        default_device = torch.device("cuda")
    elif mps_available and mps_built:
        default_device = torch.device("mps")
    else:
        default_device = torch.device("cpu")
    
    print(f"ğŸ“Œ PyTorch å½“å‰é»˜è®¤è®¡ç®—è®¾å¤‡: {default_device}")
    print("=" * 50)

if __name__ == "__main__":
    check_torch_device()